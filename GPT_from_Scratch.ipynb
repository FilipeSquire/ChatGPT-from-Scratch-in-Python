{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOEyj9LToEUUP/JAKtAJu92",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FilipeSquire/LLM-from-Scratch-in-Python/blob/main/GPT_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CHATGPT from Scratch\n",
        "\n",
        "This notebook is based on the content developed by Andrej Karpathy in his youtube channel.\n",
        "\n",
        "Its purpose is to code a Generatively Pretrained Transformer (GPT)-like model from scratch using python."
      ],
      "metadata": {
        "id": "sR8s_ZJdeblK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDzalnu8duqd",
        "outputId": "ce0f19e9-f99d-4b1a-a22e-da7b63789687"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-13 08:39:59--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-05-13 08:39:59 (42.3 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Downloading class dataset - shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "print('Length ', len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUdJyI5PhlyO",
        "outputId": "ca62a650-6814-4ad7-f11f-98269f9f4d9a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#What is the shape of the file content?\n",
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG9n2GGWhuZw",
        "outputId": "13d605ed-4823-4f93-e37a-4c08f63f8ded"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What are the unique characters in the text?\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars)) #we need .join to not print char separately\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlxuZ3hYh5-w",
        "outputId": "f7466c3b-ad9e-49a3-c485-0ba160cb1d6c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How can we tokenize?\n",
        "# tokenize means convert the raw text as a string into a series of integers\n",
        "# this is called decoding\n",
        "\n",
        "# For that we need to map our chars\n",
        "stoi = { ch:i for i, ch in enumerate(chars)} #char to int\n",
        "itos = { i:ch for i,ch in enumerate(chars)} #int to char\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s] #encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join(itos[i] for i in l) #decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode('test'))\n",
        "print(decode(encode('test')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IveI_U28iGsj",
        "outputId": "6c73551a-1c0a-4cc6-9a73-93cea19c49cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[58, 43, 57, 58]\n",
            "test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Future research:\n",
        "\n",
        "* Google uses sentencepiece (it encodes sub-words)\n",
        "* openAI uses tiktoken to do this encode and decode job"
      ],
      "metadata": {
        "id": "etLYbl3EjRJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#wrapping everything into tensors\n",
        "\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100]) #this the same we've seen before but now encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuKq5TI4jEDQ",
        "outputId": "4359bd69-bc83-4bd3-c1e2-dcdc1bc60f5c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets separate the data into train and test\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "867MigeBkNil"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we dont feed the entire text into transform\n",
        "# we will work with chunks of the dataset and feed the transformer\n",
        "\n",
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lioxd0h2kdBH",
        "outputId": "b0d10f2c-1527-481c-c7db-c0535f00df99"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# when you set a chunk of dataset\n",
        "# it actually has a chain into it\n",
        "# therefore in a chunk of 9 characteres, there is actually 8 in there\n",
        "# it happens because the chars follows a sequence\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f'when input is {context} the target is {target}')\n",
        "\n",
        "# the output reveals the 8 examples hidden in a sample of 9 chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc3a4mgAln_U",
        "outputId": "242f7b11-2c72-40e2-9a74-00c4cf44d2b6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target is 47\n",
            "when input is tensor([18, 47]) the target is 56\n",
            "when input is tensor([18, 47, 56]) the target is 57\n",
            "when input is tensor([18, 47, 56, 57]) the target is 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the second dimension to care about is the batch dimension\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 4 #number of sequences processed in parallel\n",
        "block_size = 8 #maximum content length for predictions\n",
        "\n",
        "def get_batch(split):\n",
        "  #generate small batch of data inputs x and targets y\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "\n",
        "  return x,y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "for b in range(batch_size): # we are setting a 4x8 tensor (Batch)\n",
        "  for t in range(block_size): #time dimension\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b, t]\n",
        "    print(f'when input is {context.tolist()} the target is {target}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kup_Fyi1l6jU",
        "outputId": "0a61fce7-787a-46e8-c19e-9df4244df7d4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "when input is [24] the target is 43\n",
            "when input is [24, 43] the target is 58\n",
            "when input is [24, 43, 58] the target is 5\n",
            "when input is [24, 43, 58, 5] the target is 57\n",
            "when input is [24, 43, 58, 5, 57] the target is 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target is 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target is 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target is 39\n",
            "when input is [44] the target is 53\n",
            "when input is [44, 53] the target is 56\n",
            "when input is [44, 53, 56] the target is 1\n",
            "when input is [44, 53, 56, 1] the target is 58\n",
            "when input is [44, 53, 56, 1, 58] the target is 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target is 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target is 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target is 1\n",
            "when input is [52] the target is 58\n",
            "when input is [52, 58] the target is 1\n",
            "when input is [52, 58, 1] the target is 58\n",
            "when input is [52, 58, 1, 58] the target is 46\n",
            "when input is [52, 58, 1, 58, 46] the target is 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target is 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target is 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target is 46\n",
            "when input is [25] the target is 17\n",
            "when input is [25, 17] the target is 27\n",
            "when input is [25, 17, 27] the target is 10\n",
            "when input is [25, 17, 27, 10] the target is 0\n",
            "when input is [25, 17, 27, 10, 0] the target is 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target is 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target is 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target is 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGWmKAHDsEfJ",
        "outputId": "076377f5-c8d2-42e8-aa8d-9525a00fd1d0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Research: BiGram Model"
      ],
      "metadata": {
        "id": "5YhYdZI4pQ6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# introducing ByGram Language Model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # creating a token of n-table of size vocab_size\n",
        "    # using embedding that is of shape vocab_size\n",
        "    #each token reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "\n",
        "    #idx and targets are both (B,T) tensor of integers\n",
        "    logits = self.token_embedding_table(idx) #B,T,C = batch, type, channel = 4 x 8 x 65\n",
        "    #what is happening is:\n",
        "    # predicting what comes next based on the individual identity of a single token\n",
        "    # a good way to measure loss is the negative loss likelihood\n",
        "    # pytorch cross_entropy doesnt accepts BxTxC\n",
        "    # so the solve this problem we need to do BxCxT\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T) #we need target to match logit, since it is BxT\n",
        "\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "  # now we take current token, generate and add to the previous value\n",
        "  # a prediction\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    #idx is (B,T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      #get the predictions\n",
        "      logits, loss = self(idx)\n",
        "      #focus only on the last time step\n",
        "      logits = logits[:, -1, :] #becomes B,C\n",
        "      #apply softmax to get probs\n",
        "      probs = F.softmax(logits, dim=-1) # B,C\n",
        "      #sample from distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) #B,1\n",
        "      #append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) #B, T+1\n",
        "\n",
        "    return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logs, loss = m(xb, yb)\n",
        "print(logs.shape)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo2_dMr2nsiJ",
        "outputId": "78827846-ef51-49ae-f743-687e6ccee759"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output below will be random because the model has not been trained"
      ],
      "metadata": {
        "id": "CfzSq8nWuavC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long) #creating small tensor 1x1\n",
        "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoJZi47spOo_",
        "outputId": "2926c1c6-8f9a-4111-d95f-53a74a3c61b1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a pytorch optimizer\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "mMOYMqs4uNe0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we will create a small training loop to check the code is happening"
      ],
      "metadata": {
        "id": "eJEabgo2u4I0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000):\n",
        "\n",
        "  #sample a batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  #evaluate the loss\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfNPyJ3UunAh",
        "outputId": "3171223b-d6de-420c-90d0-1523987a0fed"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5727508068084717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long) #creating small tensor 1x1\n",
        "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHskPGkuu205",
        "outputId": "fbb8a39f-d64d-4938-d527-2078ab158b1f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iyoteng h hasbe pave pirance\n",
            "Rie hicomyonthar's\n",
            "Plinseard ith henoure wounonthioneir thondy, y helti\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yey! checking again, it seems that the model started making progress.\n",
        "\n",
        "We need to make the tokens start talking between each other"
      ],
      "metadata": {
        "id": "xu0EydqPvV9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mathematics into self-attention"
      ],
      "metadata": {
        "id": "8YRdAnQjwRsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "# The problem is that we'd like the tokens to talk between each other\n",
        "# but they cant talk with any token\n",
        "# they need to talk only with the ones before it,\n",
        "# and it cannot talk with the ones in the future, because we will predict it\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 #batch, time, channel\n",
        "\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef6BVGBkvVvT",
        "outputId": "45d73bff-3c34-45d3-d404-bb0d3f09a1b3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For every token, we would like to calculate the average of all the\n",
        "# vectors in the previous tokens, and the current one\n",
        "torch.manual_seed(1337)\n",
        "# therefore we want x[b,t] = mean_{i<=t} x [b,i]\n",
        "xbow = torch.zeros((B,T,C)) #bag of words averaging\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b, :t+1] # (t,C)\n",
        "    xbow[b,t] = torch.mean(xprev, 0)\n",
        "xbow\n",
        "# The code above is a python implementation of a moving average\n",
        "# this is a costly and inefficient algorithm\n",
        "# another way of implementing the same thing in a better way\n",
        "# is a matrix multiplication"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUaTdIcdw63Y",
        "outputId": "0a7f8f22-1b70-479a-e092-b1ceef341a71"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]],\n",
              "\n",
              "        [[ 1.3488, -0.1396],\n",
              "         [ 0.8173,  0.4127],\n",
              "         [-0.1342,  0.4395],\n",
              "         [ 0.2711,  0.4774],\n",
              "         [ 0.2421,  0.0694],\n",
              "         [ 0.0084,  0.0020],\n",
              "         [ 0.0712, -0.1128],\n",
              "         [ 0.2527,  0.2149]],\n",
              "\n",
              "        [[-0.6631, -0.2513],\n",
              "         [ 0.1735, -0.0649],\n",
              "         [ 0.1685,  0.3348],\n",
              "         [-0.1621,  0.1765],\n",
              "         [-0.2312, -0.0436],\n",
              "         [-0.1015, -0.2855],\n",
              "         [-0.2593, -0.1630],\n",
              "         [-0.3015, -0.2293]],\n",
              "\n",
              "        [[ 1.6455, -0.8030],\n",
              "         [ 1.4985, -0.5395],\n",
              "         [ 0.4954,  0.3420],\n",
              "         [ 1.0623, -0.1802],\n",
              "         [ 1.1401, -0.4462],\n",
              "         [ 1.0870, -0.4071],\n",
              "         [ 1.0430, -0.1299],\n",
              "         [ 1.1138, -0.1641]]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To do this we need a triangular matrix of 1s in the same shape\n",
        "# as the matrix we will multiply.\n",
        "# Afterwards we will divide each row by its sum, so that we have the\n",
        "# weight per value of that row\n",
        "# afterwards we multiply the matrix\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0, 10, (3,2)).float()\n",
        "c = a @ b\n",
        "print('a')\n",
        "print(a)\n",
        "print('b')\n",
        "print(b)\n",
        "print('c')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3NdxerB0TCf",
        "outputId": "1af6570f-a4c5-4251-d696-a4a23179f40f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "b\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "c\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting back to the original code\n",
        "torch.manual_seed(1337)\n",
        "wei = torch.tril(torch.ones(T,T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (T,T, T) @ (B,T,C) ---> (B,T,C)"
      ],
      "metadata": {
        "id": "sBSih9Mdxs_l"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason why using softmax, is because although the future tokens will be zero, at some points they will start iterating between themselves, and we need to make sure that the current and previous tokens cannot communicate with the future tokens."
      ],
      "metadata": {
        "id": "PXbil9-t1mh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#using softmax\n",
        "torch.manual_seed(1337)\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=1)\n",
        "xbow3 = wei @ x\n",
        "xbow3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNzcB5hq0pax",
        "outputId": "c698700b-e9ab-429b-a690-b577eb667238"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]],\n",
              "\n",
              "        [[ 1.3488, -0.1396],\n",
              "         [ 0.8173,  0.4127],\n",
              "         [-0.1342,  0.4395],\n",
              "         [ 0.2711,  0.4774],\n",
              "         [ 0.2421,  0.0694],\n",
              "         [ 0.0084,  0.0020],\n",
              "         [ 0.0712, -0.1128],\n",
              "         [ 0.2527,  0.2149]],\n",
              "\n",
              "        [[-0.6631, -0.2513],\n",
              "         [ 0.1735, -0.0649],\n",
              "         [ 0.1685,  0.3348],\n",
              "         [-0.1621,  0.1765],\n",
              "         [-0.2312, -0.0436],\n",
              "         [-0.1015, -0.2855],\n",
              "         [-0.2593, -0.1630],\n",
              "         [-0.3015, -0.2293]],\n",
              "\n",
              "        [[ 1.6455, -0.8030],\n",
              "         [ 1.4985, -0.5395],\n",
              "         [ 0.4954,  0.3420],\n",
              "         [ 1.0623, -0.1802],\n",
              "         [ 1.1401, -0.4462],\n",
              "         [ 1.0870, -0.4071],\n",
              "         [ 1.0430, -0.1299],\n",
              "         [ 1.1138, -0.1641]]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Update 3"
      ],
      "metadata": {
        "id": "UVWyXeclHR1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n"
      ],
      "metadata": {
        "id": "dPSGzgeKHrWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# introducing ByGram Language Model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # creating a token of n-table of size vocab_size\n",
        "    # using embedding that is of shape vocab_size\n",
        "    #each token reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embeddin_table = nn.Embedding(block_size, n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "  def forward(self, idx, targets=None):\n",
        "\n",
        "    #idx and targets are both (B,T) tensor of integers\n",
        "    tok_emb = self.token_embedding_table(idx) #B,T,C = batch, type, channel = 4 x 8 x 65\n",
        "    logits = self.lm_head(tok_emb) #  Bt,C,vocab_size\n",
        "    #what is happening is:\n",
        "    # predicting what comes next based on the individual identity of a single token\n",
        "    # a good way to measure loss is the negative loss likelihood\n",
        "    # pytorch cross_entropy doesnt accepts BxTxC\n",
        "    # so the solve this problem we need to do BxCxT\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T) #we need target to match logit, since it is BxT\n",
        "\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "  # now we take current token, generate and add to the previous value\n",
        "  # a prediction\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    #idx is (B,T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      #get the predictions\n",
        "      logits, loss = self(idx)\n",
        "      #focus only on the last time step\n",
        "      logits = logits[:, -1, :] #becomes B,C\n",
        "      #apply softmax to get probs\n",
        "      probs = F.softmax(logits, dim=-1) # B,C\n",
        "      #sample from distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) #B,1\n",
        "      #append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) #B, T+1\n",
        "\n",
        "    return idx\n",
        "\n",
        "m = BigramLanguageModel()\n",
        "logs, loss = m(xb, yb)\n",
        "print(logs.shape)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "CvtuNJdT1aGc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i_jz4SMCHqI_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}